---
apiVersion: v1
kind: Template
labels:
  app: "${APP_NAME}-${JOB_NAME}"
  template: "${REPO_NAME}-app-dc-template"
metadata:
  name: "${REPO_NAME}-app-dc"
objects:
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      name: fluent-bit-config
      namespace: ${NAMESPACE}
    data:
      fluent-bit.conf: |-
        [SERVICE]
          Flush        5
          Daemon       Off
          # define the log format (see additional config map key/value)
          Parsers_File  parsers.conf
          Log_Level    info
          HTTP_Server   On
          HTTP_Listen   0.0.0.0
          HTTP_Port     2020

        [INPUT]
          # get logs from file written by node app (eg: CDOGS)
          Name   tail
          Path   /var/log/*
          Tag    app
          Offset_Key  logFileOffset
          Path_Key    logFilePath

        [FILTER]
          # exclude kube probe logs from app logs
          name   grep
          match  app
          Exclude  agent kube*

        [FILTER]
          name parser
          match app
          Key_Name log
          Parser json
          Reserve_Data On
          Preserve_Key On

        [FILTER]
          # modify log entry to include more key/value pairs
          name    record_modifier
          match   app
          # add pod name
          Record  hostname ${HOSTNAME}
          # add productname (eg: 'cdogs')
          Record  product ${APP_NAME}
          # add namespace
          Record namespace ${NAMESPACE}

        [FILTER]
          Name          rewrite_tag
          Match         app
          Rule          $level ([a-zA-Z]*)$ $TAG.$level true
          Emitter_Name  re_emitted

        # for now just send out http level ('access') logs to AWS
        [FILTER]
          Name    lua
          Match   app.*
          script  script.lua
          time_as_table True
          call    ecsMap

        # Note: only currently sending 'access' (level: http) logs to AWS
        # TODO: format 'metrics' logs to match a 'fingerprint' in Lambda
        [OUTPUT]
          Name kinesis_streams
          Match  app.http
          region ${AWS_DEFAULT_REGION}
          stream ${AWS_KINESIS_STREAM}
          role_arn ${AWS_ROLE_ARN}
          time_key @timestamp

        [OUTPUT]
          #### send logs to fluentd:
          Name    http
          Match   app
          Host    ${LOGGING_HOST_NAME}
          Port    80
          Format json
          # the URI becomes the Tag available in fluentd
          URI /app
          # we can also send tag as a header
          #header_tag  app
          json_date_key timestamp

          ### security:
          #tls                On
          #tls.debug         4
          #tls.verify        On
          #tls.ca_file       /fluent-bit/ssl/ca.crt.pem
          #tls.crt_file      /fluent-bit/ssl/client.crt.pem
          #tls.key_file      /fluent-bit/ssl/client.key.pem

        [OUTPUT]
          Name   stdout
          Match  *
          Format json_lines

      parsers.conf: |
        [PARSER]
          Name             json
          Format           json
          Time_Key         timestamp
          Decode_Field_as  escaped_utf8 log do_next
          Decode_Field_as  json log

      script.lua: |

        -- add extra ECS fields
        function ecsMap(tag, timestamp, record)

          -- map existing fields to a new variable
          new_record = {}

          -- derive full environment (stage) name from namespace
          -- see: https://www.lua.org/pil/20.3.html
          _, _, part1, environmentAbbreviation = string.find(record["namespace"], "([a-zA-Z0-9_+-]+)-([a-zA-Z0-9_+-]+)")

          environmentsArray = {
            ["localhost"] = "development",
            ["dev"] = "development",
            ["test"] = "test",
            ["prod"] = "production"
          }

          -- get event.type from log.level
          eventTypesArray = {
            ["http"] = "access",
            ["info"] = "info",
            ["verbose"] = "metric"
          }

          ---- for all logs:

          new_record["ecs"] = {
            ["version"] = "1.12"
          }

          new_record["log"] = {
            ["file"] = {
              ["path"] = record["logFilePath"]
            },
            ["level"] = record["level"]
          }

          new_record["service"] = {
            ["environment"] = environmentsArray[environmentAbbreviation],
            ["name"] = record["product"],
            ["type"] = "node"
          }

          new_record["event"] = {
            ["kind"] = "event",
            ["category"] = "web",
            ["original"] = record["message"],
            ["sequence"] = record["logFileOffset"],
            ["created"] = (os.date("!%Y-%m-%dT%H:%M:%S", timestamp["sec"]) .. '.' .. math.floor(timestamp["nsec"] / 1000000) .. 'Z')
          }

          new_record["agent"] = {
            ["type"] = "fluentbit",
            ["version"] = "1.8"
          }

          new_record["labels"] = {
            ["project"] = record["product"]
          }

          new_record["host"] = {
            ["hostname"] = record["hostname"],
            ["ip"] = record["ip"],
            ["namespace"] = record["namespace"]
          }

          new_record["user_agent"] = {
            ["original"] = record["userAgent"]
          }

          ---- access logs:

          if record["level"] == "http" then

            new_record["event"]["type"] = eventTypesArray[record["level"]]
            new_record["event"]["dataset"] = "express." .. eventTypesArray[record["level"]]

            new_record["http"] = {
              ["request"] = {
                ["body"] = {
                  ["bytes"] = record["contentLength"]
                },
                ["method"] = record["method"],
                ["referrer"] = {
                  ["uri"] = record["path"],
                  ["azp"] = record["azp"]
                }
              },
              ["response"] = {
                ["status_code"] = record["statusCode"],
                ["time"] = record["responseTime"]
              },
              ["version"] = record["httpVersion"]
            }

          end

          ---- metrics logs

          -- if log contains a 'metrics' field
          if record["metrics"] ~= nill then

            new_record["metrics"] = record["metrics"]
            new_record["event"]["type"] = eventTypesArray[record["level"]]

          end

          -- return the transformed new record
          return 2, timestamp, new_record
        end
parameters:
  - name: APP_NAME
    description: Application name
    displayName: Application name
    required: true
  - name: REPO_NAME
    description: Application repository name
    displayName: Repository Name
    required: true
  - name: JOB_NAME
    description: Job identifier (i.e. 'pr-5' OR 'master')
    displayName: Job Branch Name
    required: true
  - name: NAMESPACE
    description: Target namespace reference (i.e. '9f0fbe-dev')
    displayName: Target Namespace
    required: true
  - name: LOGGING_HOST_NAME
    description: The hostname of our Fluentd service running further down the monitoring pipeline
    displayName: Fluentd host name
    required: false
  - name: AWS_DEFAULT_REGION
    description: Default AWS region for Opensearch logging stack
    displayName: AWS Default Region
    required: false
  - name: AWS_KINESIS_STREAM
    description: AWS Kinesis Stream identifier
    displayName: AWS Kinesis Stream
    required: false
  - name: AWS_ROLE_ARN
    description: AWS OpenSearch/Kinesis Resource Name
    displayName: AWS Role ARN
    required: false
